{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c3aa46-ef17-4c5f-85fe-1320a277c493",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea99e2-ca92-45d6-a161-4c4138b6e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries\n",
    "!pip install PyMuPDF langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5539788-4c60-45fc-9bdb-d008c8e0d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2889239-dc86-42d8-91d6-06229c921ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a folder for sample financial PDFs\n",
    "os.makedirs(\"financial_reports\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27feb0-8575-4890-a63e-c26402333e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0a899-d44d-4434-867e-397fd5d902c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting from all pdfs\n",
    "documents = []\n",
    "pdf_folder = \"financial_reports\"\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        path = os.path.join(pdf_folder, pdf_file)           # creates full path\n",
    "        text = extract_text_from_pdf(path)\n",
    "        documents.append({\"file_name\": pdf_file, \"text\": text})\n",
    "\n",
    "print(f\"Extracted text from {len(documents)} PDFs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849651d-d46d-43fc-bde1-c32caec91abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa79953-5267-400a-8fb7-ba1d2bf60452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the text into semantic chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619367a-f425-40eb-9791-6ad43a3f4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the text\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"source_file\": doc[\"file_name\"],\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "print(f\"Total semantic chunks created: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04dfe6-fc75-49bb-800f-bfdcc2c9bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee7525-4f09-4832-bc45-904b5dda37f8",
   "metadata": {},
   "source": [
    "## embedding and vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410278d2-6e36-4260-bf53-63a10d7523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing libraries\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ab6c2-7a71-408c-9e49-57314e079b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize ChromaDB\n",
    "# Step 1: Create a persistent client\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "\n",
    "# Create or get a collection (like a table in SQL)\n",
    "collection = client.get_or_create_collection(name=\"financial_reports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d08f0e-e0bf-44cf-aced-d99c30d439d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03fe76-b944-4e1c-ab5e-2024785db884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embed and store chunks with unique IDs\n",
    "for chunk in all_chunks:\n",
    "    # Create a unique ID combining file name + chunk number\n",
    "    unique_id = f\"{chunk['source_file']}_{chunk['chunk_id']}\"\n",
    "\n",
    "    embedding = embed_model.encode(chunk[\"text\"]).tolist()\n",
    "    \n",
    "    collection.add(\n",
    "        ids=[unique_id],   # <-- REQUIRED UNIQUE ID\n",
    "        documents=[chunk[\"text\"]],\n",
    "        metadatas=[{\"source_file\": chunk[\"source_file\"], \"chunk_id\": chunk[\"chunk_id\"]}],\n",
    "        embeddings=[embedding]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e982f-598d-450b-a08b-6546891603e1",
   "metadata": {},
   "source": [
    "## local llm mistral via ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0c5b6-c4ea-40bf-bdf3-5560ab9ea693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350a1a4-d8a9-45eb-ad9b-13e42c5f25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# API endpoint\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "query = \"what day is today\"\n",
    "# Payload with prompt\n",
    "payload = {\n",
    "    \"model\": \"mistral\",\n",
    "    \"prompt\": query\n",
    "}\n",
    "\n",
    "# Send request\n",
    "response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "# Read stream response\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        data = json.loads(line.decode(\"utf-8\"))\n",
    "        if \"response\" in data:\n",
    "            print(data[\"response\"], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accc304-e537-4355-a1e8-bb105fabfac1",
   "metadata": {},
   "source": [
    "## adding embedding model with vector db and user query for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d938b-6ef4-4302-bc42-e45d44f61b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# 1. Load embedding model (local, free, open-source)\n",
    "#embed_model\n",
    "\n",
    "# 2. Connect to Chroma (vector DB)\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_collection(\"financial_reports\")\n",
    "\n",
    "# 3. Encode user query\n",
    "query = \"What was Google's total revenue in 2023?\"\n",
    "query_embedding = embed_model.encode(query).tolist()  # convert to list for chroma\n",
    "\n",
    "# 4. Search in vector DB\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3  # top-k chunks\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff4da0-1663-486f-b532-e421516543ab",
   "metadata": {},
   "source": [
    "## retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7e601-daeb-4500-a315-9fbb88e0083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Use the same embedding model that was used while inserting docs\n",
    "embedding_model = embed_model\n",
    "\n",
    "# Load the existing Chroma collection\n",
    "retriever = Chroma(\n",
    "    persist_directory=\"chroma_db\",\n",
    "    collection_name=\"financial_reports\",   # âœ… your collection name\n",
    "    embedding_function=embedding_model\n",
    ").as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d9693-7380-4f70-8b6f-bfebc2a9f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d0b91-522a-4abe-9d75-66c2888f9a1e",
   "metadata": {},
   "source": [
    "## Now RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f06c8-c14c-4fd5-96f1-159e1f1a7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d42e94-bf47-44bd-90ed-7f0607aaade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm defination\n",
    "llm = ChatOllama(model=\"mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a41a3e-f915-40c9-8c1f-be69519f17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# givng prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert financial assistant. \n",
    "Use the following context from Google's annual reports to answer the question.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3038b98-df02-417e-8ebb-9dff2f85fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the chain from langchain for proper retrivel\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e421c-236d-4f2a-b062-8894e27fd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the chain\n",
    "query = \"total 2024 revenue in billions?\"\n",
    "result = qa_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a4821-ba83-424a-a5e2-962fe429e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30e384-b526-4137-af0f-3de995d2b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the source and other details to the answer with python function\n",
    "\n",
    "# Function to query with provenance\n",
    "def rag_query_with_sources(question):\n",
    "    # Step 1: Retrieve relevant documents (chunks)\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Step 2: Build context with source info\n",
    "    context_with_sources = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        source_name = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "        page_number = doc.metadata.get(\"chunk_id\", \"unknown\")\n",
    "        snippet = doc.page_content[:100]  # first 300 chars\n",
    "        context_with_sources += f\"[Source: {source_name}, Page: {page_number}] {snippet}\\n\\n\"\n",
    "\n",
    "    # Step 3: final prompt\n",
    "    final_prompt = f\"\"\"\n",
    "    You are an expert financial assistant. \n",
    "    Use the following context to answer the question.\n",
    "   \n",
    "\n",
    "    Context:\n",
    "    {context_with_sources}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 4: Query Mistral via Ollama\n",
    "    response = llm.invoke(final_prompt)\n",
    "\n",
    "    # Step 5: Return both answer and sources (structured)\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"sources\": [{\"source\": doc.metadata.get(\"source_file\"),\n",
    "                     \"chunk_no\": doc.metadata.get(\"chunk_id\"),\n",
    "                     \"snippet\": doc.page_content[:200]} for doc in docs]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb302014-ed4c-406f-ba96-117143bbc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing this function\n",
    "result = rag_query_with_sources(\"What was Google's revenue in 2023 compared to 2022?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fa2ed-951a-4cc0-a5e7-29184c1e87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result[\"answer\"])\n",
    "print(result[\"sources\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a0aec-5e66-4e7d-85e5-b3eb22fc032f",
   "metadata": {},
   "source": [
    "## Adding another dataset in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac4ad0-dabf-4454-96c5-92690189240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "csv_file = \"books./GOOGL-2013_2023.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Inspect first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b13765-9142-4b0b-a74e-d1b933a3df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv into shunks\n",
    "\n",
    "all_stock_chunks = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text_chunk = f\"Date: {row['Date']}, Open: {row['Open']}, High: {row['High']}, Low: {row['Low']}, Close: {row['Close']}, Volume: {row['Volume']}\"\n",
    "    \n",
    "    all_stock_chunks.append({\n",
    "        \"text\": text_chunk,\n",
    "        \"source_file\": \"google_stock_prices.csv\",\n",
    "        \"chunk_id\": idx\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a8703-ca1f-404a-85dd-6d8929fd3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating collection\n",
    "stock_collection = client.create_collection(name=\"stock_prices\")\n",
    "\n",
    "# adding chunks\n",
    "for chunk in all_stock_chunks:\n",
    "    unique_id = f\"{chunk['source_file']}_{chunk['chunk_id']}\"\n",
    "    \n",
    "    embedding = embed_model.encode(chunk[\"text\"]).tolist()\n",
    "    \n",
    "    stock_collection.add(\n",
    "        ids=[unique_id],\n",
    "        documents=[chunk[\"text\"]],\n",
    "        metadatas=[{\"source_file\": chunk[\"source_file\"], \"chunk_id\": chunk[\"chunk_id\"]}],\n",
    "        embeddings=[embedding]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b074a2a-7be0-422d-a859-75021e629db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all collections in the Chroma client\n",
    "collections = client.list_collections()\n",
    "print(\"Available collections:\")\n",
    "for col in collections:\n",
    "    print(\"-\", col.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae128e-c2f7-4445-9fff-4a9d54281623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriver for this collection\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Use the same embedding model that was used while inserting docs\n",
    "embedding_model = embed_model\n",
    "\n",
    "# Load the existing Chroma collection\n",
    "retriever_stock = Chroma(\n",
    "    persist_directory=\"chroma_db\",\n",
    "    collection_name=\"stock_prices\",   # âœ… collection name\n",
    "    embedding_function=embedding_model\n",
    ").as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996ee28-e805-4d27-b2e5-1be9c8abb259",
   "metadata": {},
   "source": [
    "## Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a95a5-63cb-4258-8697-e829d74a42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_planner(query: str) -> str:\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Keywords for annual reports (financial info)\n",
    "    financial_keywords = [\"revenue\", \"profit\", \"expenses\", \"growth\", \"financial\", \"earnings\", \"income\"]\n",
    "    \n",
    "    # Keywords for stock prices (CSV dataset)\n",
    "    stock_keywords = [\"stock\", \"share price\", \"close price\", \"open price\", \"high\", \"low\", \"volume\"]\n",
    "    \n",
    "    # Check if the query matches financial keywords\n",
    "    if any(word in query_lower for word in financial_keywords):\n",
    "        return \"annual_reports\"\n",
    "    \n",
    "    # Check if the query matches stock keywords\n",
    "    elif any(word in query_lower for word in stock_keywords):\n",
    "        return \"stock_prices\"\n",
    "    \n",
    "    # Fallback if unsure\n",
    "    else:\n",
    "        return \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433e67b-0765-4eb5-9109-219cf705e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever_docs(user_query, retriever_reports, retriever_stock):\n",
    "# --- Planner ---\n",
    "    collection_choice = simple_planner(user_query)\n",
    "    \n",
    "    # --- Retriever ---\n",
    "    if collection_choice == \"annual_reports\":\n",
    "        docs = retriever_reports.get_relevant_documents(user_query)\n",
    "    elif collection_choice == \"stock_prices\":\n",
    "        docs = retriever_stock.get_relevant_documents(user_query)\n",
    "    else:  # fallback to combine both\n",
    "        docs_reports = retriever_reports.get_relevant_documents(user_query)\n",
    "        docs_stock = retriever_stock.get_relevant_documents(user_query)\n",
    "        docs = docs_reports + docs_stock\n",
    "    \n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44c68e-3f56-4527-bcfe-031acf1b576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llm_prompt(docs, user_query):\n",
    " # --- Step 1: Build context with sources ---\n",
    "    context_with_sources = \"\"\n",
    "    for doc in docs:\n",
    "        source_name = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\", \"unknown\")\n",
    "        snippet = doc.page_content[:300]  # first 300 characters\n",
    "        context_with_sources += f\"[Source: {source_name}, Chunk: {chunk_id}] {snippet}\\n\\n\"\n",
    "\n",
    "    # --- Step 2: Build prompt ---\n",
    "    prompt = f\"\"\"\n",
    "You are an expert financial assistant. \n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context_with_sources}\n",
    "\n",
    "Question:\n",
    "{user_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8cea3-41f3-475c-a0f9-90b8b25c8e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439e2cc-89d5-4d1e-9ee5-d57a42d14da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Revenue in billion in 2023\"\n",
    "docs = get_retriever_docs(user_query, retriever, retriever_stock)\n",
    "\n",
    "prompt = build_llm_prompt(docs, user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff18e32-8baa-4cdd-9a59-1c23dabdd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using llm mistral for this. with planner retriver and context\n",
    "response = llm.invoke(prompt)\n",
    "answer = response.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60dbcc-ad77-4c47-b0cb-db15d88573f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a66bad-7d92-4a92-a2d0-92ea9c2bb231",
   "metadata": {},
   "source": [
    "## logging to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777021a-957d-4336-8c42-63405d03e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def log_llm_run(user_query, answer, docs, plan, log_file=\"llm_logs.txt\"):\n",
    "# Step 1: Get current timestamp\n",
    "    current_time = datetime.datetime.now()\n",
    "\n",
    "    # Step 2: Start building the log string\n",
    "    log_entry = f\"--- LLM Run: {current_time} ---\\n\"\n",
    "    log_entry += f\"User Query: {user_query}\\n\"\n",
    "    log_entry += f\"Collection Used: {plan}\\n\"\n",
    "    log_entry += f\"Answer: {answer}\\n\"\n",
    "    log_entry += \"Sources Used:\\n\"\n",
    "\n",
    "    # Step 3: Add each document's source info\n",
    "    for doc in docs:\n",
    "        source_file = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\", \"unknown\")\n",
    "        log_entry += f\"  - Source File: {source_file}, Chunk ID: {chunk_id}\\n\"\n",
    "\n",
    "    log_entry += \"\\n\\n\"  # blank line at end for separation\n",
    "\n",
    "    # Step 4: Write log to the file\n",
    "    with open(log_file, \"a\") as file:\n",
    "        file.write(log_entry)\n",
    "\n",
    "    # Step 5: Return the log string so it can be printed or used immediately\n",
    "    return log_entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353dd4b7-ff85-4e8c-840a-bf565de86df5",
   "metadata": {},
   "source": [
    "## Now complete run with planner retriver logfiles and llm synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b384f9d-9bd0-4f6c-afcb-d21d02451a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(user_query):\n",
    "    plan = simple_planner(user_query)\n",
    "    docs = get_retriever_docs(user_query, retriever, retriever_stock)\n",
    "    prompt = build_llm_prompt(docs, user_query)\n",
    "    response = llm.invoke(prompt)\n",
    "    answer = response.content\n",
    "    if not is_high_confidence(docs, threshold=0.6):\n",
    "        approved = human_review(answer)\n",
    "        if not approved:\n",
    "            answer = \"[ESCALATED] Awaiting human input.\"\n",
    "    log = log_llm_run(user_query,answer,docs,plan)\n",
    "    \n",
    "    return answer,log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7f153-de09-4d72-b674-99ee4652dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = input(\"Enter your query\")\n",
    "answer,log = run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77f067-0cf5-4bd3-81ec-7315397ec5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Response:{answer}\\n\\n\")\n",
    "print(f\"log file:{log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa61f6e-93e2-4842-a917-20db00ee1430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84fbe26-7eaa-4cd2-b985-df0d846eafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building confidence thershold function\n",
    "\n",
    "def is_high_confidence(docs, threshold=0.6):\n",
    "    if not docs:\n",
    "        return False\n",
    "    # Assuming each doc.metadata has a 'similarity' score (0-1)\n",
    "    top_score = max(doc.metadata.get(\"similarity\", 0) for doc in docs)\n",
    "    return top_score >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e67d34-e153-4f5d-9ce7-6e6afd25a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## building for human review \n",
    "\n",
    "def human_review(answer):\n",
    "    print(\"\\n--- HUMAN REVIEW REQUIRED ---\")\n",
    "    print(\"Suggested answer:\\n\", answer)\n",
    "    confirm = input(\"\\nDo you approve this answer? (y/n): \").strip().lower()\n",
    "    if confirm == \"y\":\n",
    "        print(\"Answer approved.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Answer escalated!\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aefcaf-1902-4cc8-83a0-5ab94a5c2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    huggingfacehub_api_token=HF_API_KEY\n",
    ")\n",
    "\n",
    "prompt = \"Summarize the following text: Google made $XYZ revenue in 2023.\"\n",
    "\n",
    "# Call directly\n",
    "answer = llm(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044bd063-7d74-4ae2-9c22-98f9aaa0d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain langchain-huggingface huggingface_hub transformers sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e455b-b7d7-4cec-91db-7608bb94cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Summarize: Google revenue in 2023.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb16aeb-200c-455a-b019-23afddd44f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/google/flan-t5-large\",\n",
    "    huggingfacehub_api_token=HF_API_KEY)\n",
    "\n",
    "\n",
    "prompt = \"Summarize: Google revenue in 2023.\"\n",
    "\n",
    "# If HuggingFaceEndpoint\n",
    "answer = llm(prompt)\n",
    "print(answer)\n",
    "\n",
    "# If HuggingFaceHub\n",
    "answer = llm(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370fc65-88a4-402b-b2a9-21120ae0f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Hugging Face API key\"\n",
    "\n",
    "# Create the Hugging Face endpoint with the API key passed directly\n",
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\", huggingfacehub_api_token=api_key)\n",
    "\n",
    "# Generate text\n",
    "output = llm.predict(\"Write a short poem on India\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbf9e2-ff1f-4d93-a6c8-76f8e2687c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    huggingfacehub_api_token=api_key,\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "print(llm(\"Write a short poem on India\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ae06c-1f00-45a7-a7c9-2886ecef8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub==0.13.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde0044-8011-433f-8ac6-496b87d684e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    huggingfacehub_api_token=api_key,\n",
    "    task=\"text-generation\",\n",
    ")\n",
    "\n",
    "print(llm(\"Write a short poem on India\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f6f18-cca6-4141-873a-6575592fa0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub==0.15.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e625dc-0b88-4451-87b0-07c9f97e3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    huggingfacehub_api_token=api_key,\n",
    "    task=\"text-generation\",\n",
    ")\n",
    "\n",
    "print(llm(\"Write a short poem on India\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6815b-30eb-479c-9e4f-4ef716cf87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall huggingface_hub langchain langchain-huggingface -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6687c5b-4290-4639-99d5-4f35b51c3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub==0.15.1\n",
    "!pip install langchain==0.0.174\n",
    "!pip install langchain_huggingface==0.3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba8f43-a47d-4442-b3f0-d8e9e74e92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-huggingface\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (6th-sense-intern-conda)",
   "language": "python",
   "name": "6th-sense-intern-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
